# >>> Dataset
root_path = "./data"
# ml-1m / ml-100k
dataset = 'ml-1m'
train_ratio = 0.9

# >>> Network
hidden_units = 500
lambda = 0.5

# >>> Training
cuda = false
seed = 5760 # random seed
epochs = 50
batch_size = 100
grad_clip = false # clip the grad to [1, 5]
base_lr = 1e-3
decay_step = 50 # decay the learning rate for each n steps
log_step = 20
optimizer = "Adam" # Adam / RMSProp / SGD

# >>> Logger
console_output = true
log_root = "./log"
log_name = "autorec"
append_time = true
